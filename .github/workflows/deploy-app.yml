# .github/workflows/deploy-app.yml
# PHP 애플리케이션 배포 워크플로우 (server 폴더 변경시만 실행)

name: Deploy PHP Application

on:
  push:
    branches: [main]
    paths:
      - "WALB/server/**"
  pull_request:
    branches: [main]
    paths:
      - "WALB/server/**"

env:
  PROJECT_NAME: "walb-app"

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    # 워킹 디렉토리를 WALB로 설정
    defaults:
      run:
        working-directory: ./WALB

    permissions:
      id-token: write
      contents: read

    steps:
      # ===============================================
      # 소스코드 체크아웃
      # ===============================================
      - name: Checkout code
        uses: actions/checkout@v4

      # ===============================================
      # PHP 및 Composer 환경 설정
      # ===============================================
      - name: Set up PHP
        uses: shivammathur/setup-php@v2
        with:
          php-version: "8.1"
          extensions: pdo, pdo_pgsql, mbstring, xml, zip, gd
          coverage: none

      - name: Validate Composer
        run: |
          echo "🔍 PHP 애플리케이션 검증 중..."
          if [ -f "server/composer.json" ]; then
            cd server
            composer validate --no-check-publish
            composer install --no-dev --optimize-autoloader --no-interaction
            echo "✅ Composer 검증 완료"
          else
            echo "ℹ️ Composer 파일이 없습니다. Docker 빌드만 실행합니다."
          fi

      # ===============================================
      # AWS 인증 (OIDC 방식)
      # ===============================================
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN_APP }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-session-name: GitHubActions-Application-${{ github.run_id }}

      # ===============================================
      # 기존 인프라 정보 조회
      # ===============================================
      - name: Get Infrastructure Resources
        run: |
          echo "🔍 기존 인프라 리소스 정보 조회 중..."

          # ECR 리포지토리 URI 조회
          ECR_REPO=$(aws ecr describe-repositories --repository-names ${PROJECT_NAME}-ecr --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "")
          if [ -z "$ECR_REPO" ]; then
            echo "❌ ECR 리포지토리를 찾을 수 없습니다: ${PROJECT_NAME}-ecr"
            echo "먼저 인프라 배포가 필요합니다."
            exit 1
          fi
          echo "ECR_REPOSITORY=$ECR_REPO" >> $GITHUB_ENV
          echo "✅ ECR Repository: $ECR_REPO"

          # EKS 클러스터 이름 조회
          EKS_CLUSTER=$(aws eks describe-cluster --name walb-eks-cluster --query 'cluster.name' --output text 2>/dev/null || echo "")
          if [ -z "$EKS_CLUSTER" ] || [ "$EKS_CLUSTER" == "None" ]; then
            echo "❌ EKS 클러스터를 찾을 수 없습니다: ${PROJECT_NAME}-eks"
            echo "먼저 인프라 배포가 필요합니다."
            exit 1
          fi
          echo "EKS_CLUSTER_NAME=$EKS_CLUSTER" >> $GITHUB_ENV
          echo "✅ EKS Cluster: $EKS_CLUSTER"

          # RDS 엔드포인트 조회
          RDS_ENDPOINT=$(aws rds describe-db-instances --query 'DBInstances[?DBName==`mydb`].Endpoint.Address' --output text 2>/dev/null || echo "")
          if [ -z "$RDS_ENDPOINT" ] || [ "$RDS_ENDPOINT" == "None" ]; then
            echo "❌ RDS 인스턴스를 찾을 수 없습니다"
            echo "먼저 인프라 배포가 필요합니다."
            exit 1
          fi
          echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
          echo "✅ RDS Endpoint: $RDS_ENDPOINT"

          # EKS 클러스터 상태 확인
          EKS_STATUS=$(aws eks describe-cluster --name $EKS_CLUSTER --query 'cluster.status' --output text)
          if [ "$EKS_STATUS" != "ACTIVE" ]; then
            echo "❌ EKS 클러스터가 활성 상태가 아닙니다: $EKS_STATUS"
            exit 1
          fi
          echo "✅ EKS Cluster Status: $EKS_STATUS"

      # ===============================================
      # ECR 로그인
      # ===============================================
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      # ===============================================
      # Docker 이미지 빌드 및 푸시
      # ===============================================
      - name: Build and push Docker image
        id: build-image
        run: |
          echo "🐳 Docker 이미지 빌드 중..."

          # Git 커밋 해시를 태그로 사용
          IMAGE_TAG=${{ github.sha }}
          IMAGE_URI=${{ env.ECR_REPOSITORY }}:$IMAGE_TAG

          # server 폴더로 이동해서 Docker 빌드
          cd server
          docker build -t $IMAGE_URI .
          docker tag $IMAGE_URI ${{ env.ECR_REPOSITORY }}:latest

          echo "📤 ECR에 이미지 푸시 중..."
          docker push $IMAGE_URI
          docker push ${{ env.ECR_REPOSITORY }}:latest

          echo "✅ 이미지 푸시 완료: $IMAGE_URI"
          echo "image=$IMAGE_URI" >> $GITHUB_OUTPUT

      - name: Database Connection and Schema Setup
        run: |
          # AWS CLI를 사용해서 리소스 정보 직접 조회
          PROJECT_NAME="walb-app"

          # RDS 엔드포인트 조회 (태그 기반)
          echo "🔍 RDS 인스턴스 조회 중..."
          DB_HOST=$(aws rds describe-db-instances \
            --query "DBInstances[?contains(keys(TagList[?Key=='Project']), 'Project') && TagList[?Key=='Project'].Value[0]=='${PROJECT_NAME}'].Endpoint.Address" \
            --output text 2>/dev/null || echo "")

          if [ -z "$DB_HOST" ]; then
            # 태그 조회가 안 되면 DB 이름으로 조회
            DB_HOST=$(aws rds describe-db-instances \
              --query "DBInstances[?DBName=='mydb'].Endpoint.Address" \
              --output text 2>/dev/null || echo "")
          fi

          # Bastion Host IP 조회 (태그 기반)
          echo "🔍 Bastion Host 조회 중..."
          BASTION_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=${PROJECT_NAME}-bastion-host" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text 2>/dev/null || echo "")

          if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
            # 태그로 안 되면 보안그룹으로 조회
            BASTION_IP=$(aws ec2 describe-instances \
              --filters "Name=tag:Component,Values=Bastion" "Name=instance-state-name,Values=running" \
              --query "Reservations[0].Instances[0].PublicIpAddress" \
              --output text 2>/dev/null || echo "")
          fi

          # DB 사용자명과 DB 이름 (하드코딩된 값 사용)
          DB_NAME="mydb"
          DB_USER="dbadmin"

          # Parameter Store에서 DB 패스워드 조회
          echo "🔍 DB 패스워드 조회 중..."
          DB_PASSWORD=$(aws ssm get-parameter \
            --name "/${PROJECT_NAME}/rds/master-password" \
            --with-decryption \
            --query 'Parameter.Value' \
            --output text 2>/dev/null || echo "")

          # 값 검증
          if [ -z "$DB_HOST" ] || [ "$DB_HOST" == "None" ]; then
            echo "❌ RDS 엔드포인트를 찾을 수 없습니다."
            echo "사용 가능한 RDS 인스턴스:"
            aws rds describe-db-instances --query "DBInstances[*].[DBInstanceIdentifier,Endpoint.Address,DBName]" --output table
            exit 1
          fi

          if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
            echo "❌ Bastion Host를 찾을 수 없습니다."
            echo "실행 중인 EC2 인스턴스:"
            aws ec2 describe-instances \
              --filters "Name=instance-state-name,Values=running" \
              --query "Reservations[*].Instances[*].[InstanceId,PublicIpAddress,Tags[?Key=='Name'].Value[0]]" \
              --output table
            exit 1
          fi

          if [ -z "$DB_PASSWORD" ]; then
            echo "❌ DB 패스워드를 Parameter Store에서 찾을 수 없습니다."
            exit 1
          fi

          echo "✅ DB Host: '$DB_HOST'"
          echo "✅ Bastion IP: '$BASTION_IP'"
          echo "✅ DB User: '$DB_USER'"
          echo "✅ DB Name: '$DB_NAME'"

          # SSH 키를 Parameter Store에서 가져오기
          echo "🔑 SSH 키 조회 중..."
          aws ssm get-parameter \
            --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --output text > bastion_key.pem
          chmod 600 bastion_key.pem

          echo "🔍 SSH 연결 테스트 중..."
          # SSH 연결 테스트 (타임아웃 설정)
          if ! ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP "echo 'SSH connection successful'" 2>/dev/null; then
            echo "❌ SSH 연결 실패. Bastion Host 상태 확인:"
            aws ec2 describe-instances \
              --filters "Name=tag:Component,Values=Bastion" \
              --query "Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress,PrivateIpAddress]" \
              --output table
            exit 1
          fi

          echo "🔗 SSH 터널을 통한 PostgreSQL 연결 테스트 중..."

          # Bastion Host에서 RDS 연결 테스트 먼저 수행
          echo "🔍 Bastion Host에서 RDS 직접 연결 테스트 중..."
          echo "  DB Host: $DB_HOST"
          echo "  DB Port: 5432"

          # netcat 설치 및 연결 테스트
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP \
            "command -v nc >/dev/null 2>&1 || sudo yum install -y nc; echo 'Testing connection...'; timeout 10 nc -zv $DB_HOST 5432" || {
            echo "⚠️ Bastion에서 RDS 직접 연결 실패"
            echo "대체 연결 테스트 시도 중..."
            
            # telnet 대체 테스트
            ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP \
              "timeout 10 bash -c 'exec 3<>/dev/tcp/$DB_HOST/5432' && echo 'Raw socket connection successful' || echo 'Raw socket connection failed'"
          }

          # 보안 그룹 정보 확인
          echo "🔍 보안 그룹 정보 확인 중..."
          BASTION_SG=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=${PROJECT_NAME}-bastion-host" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" --output text)

          RDS_SG=$(aws rds describe-db-instances \
            --query "DBInstances[?DBName=='mydb'].VpcSecurityGroups[0].VpcSecurityGroupId" --output text)

          echo "Bastion Security Group: $BASTION_SG"
          echo "RDS Security Group: $RDS_SG"

          # 보안 그룹 규칙 확인 (권한이 있는 경우에만)
          if [ -n "$RDS_SG" ]; then
            echo "🔍 RDS 보안 그룹 인바운드 규칙 확인 시도..."
            if aws ec2 describe-security-groups --group-ids "$RDS_SG" \
              --query "SecurityGroups[0].IpPermissions[?FromPort==\`5432\`]" --output table 2>/dev/null; then
              echo "✅ 보안 그룹 규칙 조회 성공"
            else
              echo "⚠️ 보안 그룹 규칙 조회 권한 없음 (정상 - 보안상 제한)"
              echo "RDS Security Group ID: $RDS_SG"
              echo "Bastion Security Group ID: $BASTION_SG"
            fi
          fi

          # SSH 터널을 통한 PostgreSQL 연결 테스트
          echo "🔗 SSH 터널 생성 중..."
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ExitOnForwardFailure=yes -L 5432:$DB_HOST:5432 ec2-user@$BASTION_IP -N &
          SSH_PID=$!

          # 터널 설정 대기
          sleep 15

          # PostgreSQL 클라이언트 설치 (필요한 경우)
          if ! command -v psql &> /dev/null; then
            echo "📦 PostgreSQL 클라이언트 설치 중..."
            sudo apt-get update && sudo apt-get install -y postgresql-client
          fi

          # 로컬 포트를 통해 RDS 연결 테스트
          if PGPASSWORD=$DB_PASSWORD psql -h localhost -p 5432 -U $DB_USER -d $DB_NAME -c "SELECT 1;" 2>/dev/null; then
            echo "✅ 데이터베이스 연결 성공"
          else
            echo "❌ 데이터베이스 연결 실패"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi

          # SSH 터널 종료 및 정리
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem

          echo "✅ 데이터베이스 연결 테스트 완료"
        env:
          PROJECT_NAME: "walb-app"

      # ===============================================
      # PostgreSQL 클라이언트 설치 및 스키마 적용
      # ===============================================
      - name: Apply Database Schema
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "🗄️ 데이터베이스 스키마 적용 중..."

          # PostgreSQL 클라이언트 설치
          sudo apt-get update && sudo apt-get install -y postgresql-client

          # 환경 변수 설정
          PROJECT_NAME="walb-app"
          DB_HOST="${{ env.RDS_ENDPOINT }}"
          DB_NAME="mydb"
          DB_USER="dbadmin"

          # Bastion Host IP 조회
          BASTION_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=${PROJECT_NAME}-bastion-host" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text 2>/dev/null || echo "")

          if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
            echo "❌ Bastion Host를 찾을 수 없습니다."
            exit 1
          fi

          echo "🔍 연결 정보:"
          echo "  RDS 엔드포인트: $DB_HOST"
          echo "  Bastion IP: $BASTION_IP"
          echo "  DB 이름: $DB_NAME"
          echo "  DB 사용자: $DB_USER"

          # SSH 키를 Parameter Store에서 가져오기
          echo "🔑 SSH 키 가져오는 중..."
          aws ssm get-parameter \
            --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --output text > bastion_key.pem
          chmod 600 bastion_key.pem

          # SSH 터널 생성 (백그라운드에서 실행)
          echo "🔗 SSH 터널 생성 중..."
          ssh -i bastion_key.pem \
              -o StrictHostKeyChecking=no \
              -o ExitOnForwardFailure=yes \
              -L 5432:$DB_HOST:5432 \
              ec2-user@$BASTION_IP \
              -N &
          SSH_PID=$!

          # 터널 설정 대기
          echo "⏳ SSH 터널 설정 대기 중..."
          sleep 15

          # SSH 터널 상태 확인
          if ! kill -0 $SSH_PID 2>/dev/null; then
            echo "❌ SSH 터널 생성 실패"
            rm -f bastion_key.pem
            exit 1
          fi

          echo "✅ SSH 터널 생성 완료"

          # 로컬 포트를 통해 RDS 연결 테스트
          echo "🔍 데이터베이스 연결 테스트 중..."
          if ! PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
            -h localhost \
            -p 5432 \
            -U "$DB_USER" \
            -d "$DB_NAME" \
            -c "SELECT version();" \
            -v ON_ERROR_STOP=1 >/dev/null 2>&1; then
            echo "❌ 데이터베이스 연결 실패"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi

          echo "✅ 데이터베이스 연결 성공"

          # 기존 테이블 확인
          echo "🔍 기존 테이블 확인 중..."
          EXISTING_TABLES=$(PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
            -h localhost \
            -p 5432 \
            -U "$DB_USER" \
            -d "$DB_NAME" \
            -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name IN ('users', 'posts', 'images', 'files');" \
            -v ON_ERROR_STOP=1 | tr -d ' ')

          if [ "$EXISTING_TABLES" -eq "0" ]; then
            echo "📝 스키마 파일 적용 중..."
            if PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
              -h localhost \
              -p 5432 \
              -U "$DB_USER" \
              -d "$DB_NAME" \
              -f server/files/schema.sql \
              -v ON_ERROR_STOP=1; then
              echo "✅ 스키마 적용 완료"
            else
              echo "❌ 스키마 적용 실패"
              kill $SSH_PID 2>/dev/null
              rm -f bastion_key.pem
              exit 1
            fi
          else
            echo "ℹ️ 테이블이 이미 존재합니다. 스키마 적용을 건너뜁니다."
          fi

          # 정리 작업
          echo "🧹 정리 작업 중..."
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem

          echo "✅ 데이터베이스 스키마 작업 완료"

      # ===============================================
      # kubectl 및 Helm 설치
      # ===============================================
      - name: Install kubectl and Helm
        run: |
          echo "🔧 kubectl 및 Helm 설치 중..."

          # kubectl 설치
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Helm 설치
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

          echo "✅ kubectl 및 Helm 설치 완료"
          kubectl version --client
          helm version

      - name: Update kubeconfig for EKS
        run: |
          echo "🔧 EKS 클러스터 kubeconfig 업데이트 중..."

          # 현재 AWS 자격 증명 확인
          echo "🔍 현재 AWS 자격 증명 확인..."
          aws sts get-caller-identity

          # EKS 클러스터 이름 조회 (여러 방법 시도)
          echo "🔍 EKS 클러스터 조회 중..."

          # 방법 1: 클러스터 목록에서 첫 번째 조회
          EKS_CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")

          # 방법 2: 특정 이름으로 조회
          if [ -z "$EKS_CLUSTER_NAME" ] || [ "$EKS_CLUSTER_NAME" == "None" ]; then
            EKS_CLUSTER_NAME="walb-eks-cluster"
            echo "기본 클러스터 이름 사용: $EKS_CLUSTER_NAME"
          fi

          # 클러스터 존재 확인
          if ! aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} >/dev/null 2>&1; then
            echo "❌ EKS 클러스터 '$EKS_CLUSTER_NAME'를 찾을 수 없습니다."
            echo "사용 가능한 클러스터 목록:"
            aws eks list-clusters --region ${{ secrets.AWS_REGION }}
            exit 1
          fi

          echo "✅ EKS 클러스터: $EKS_CLUSTER_NAME"
          echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV

          # 클러스터 상태 확인
          CLUSTER_STATUS=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.status' --output text)
          echo "클러스터 상태: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "❌ 클러스터가 ACTIVE 상태가 아닙니다: $CLUSTER_STATUS"
            exit 1
          fi

          # kubeconfig 업데이트 (상세 로그 포함)
          echo "🔧 kubeconfig 업데이트 중..."
          aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name "$EKS_CLUSTER_NAME" --verbose

          # 클러스터 연결 테스트 (자세한 오류 정보 포함)
          echo "🔍 클러스터 연결 테스트..."
          if ! kubectl cluster-info --request-timeout=30s; then
            echo "❌ kubectl cluster-info 실패. 추가 진단 정보:"
            
            # kubectl 설정 확인
            echo "kubectl 설정 확인:"
            kubectl config view
            
            # 현재 컨텍스트 확인
            echo "현재 컨텍스트:"
            kubectl config current-context
            
            # 클러스터 엔드포인트 직접 테스트
            CLUSTER_ENDPOINT=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.endpoint' --output text)
            echo "클러스터 엔드포인트: $CLUSTER_ENDPOINT"
            
            # IAM 역할과 RBAC 매핑 확인
            echo "EKS 클러스터의 aws-auth ConfigMap 확인이 필요할 수 있습니다."
            exit 1
          fi

          echo "🔍 노드 상태 확인..."
          kubectl get nodes --show-labels

      # ===============================================
      # AWS Load Balancer Controller 정리 및 준비
      # ===============================================
      - name: Clean up and prepare for AWS Load Balancer Controller
        run: |
          echo "🧹 기존 AWS Load Balancer Controller 리소스 정리 중..."

          # 기존 AWS Load Balancer Controller 제거
          if helm list -n kube-system | grep -q aws-load-balancer-controller; then
            echo "기존 AWS Load Balancer Controller 제거 중..."
            helm uninstall aws-load-balancer-controller -n kube-system || true
            sleep 10
          fi

          # 웹훅 관련 리소스 정리
          kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || echo "ValidatingAdmissionWebhook 없음"
          kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || echo "MutatingAdmissionWebhook 없음"
          kubectl delete service aws-load-balancer-webhook-service -n kube-system 2>/dev/null || echo "웹훅 서비스 없음"

          # cert-manager 완전 제거 (더 이상 사용하지 않음)
          echo "🗑️ cert-manager 완전 제거 중..."
          kubectl delete namespace cert-manager 2>/dev/null || true
          kubectl delete crd certificaterequests.cert-manager.io 2>/dev/null || true
          kubectl delete crd certificates.cert-manager.io 2>/dev/null || true
          kubectl delete crd challenges.acme.cert-manager.io 2>/dev/null || true
          kubectl delete crd clusterissuers.cert-manager.io 2>/dev/null || true
          kubectl delete crd issuers.cert-manager.io 2>/dev/null || true
          kubectl delete crd orders.acme.cert-manager.io 2>/dev/null || true
          helm uninstall cert-manager -n cert-manager 2>/dev/null || true
          
          echo "✅ 기존 리소스 정리 완료"

      # ===============================================
      # AWS Load Balancer Controller 설치 (cert-manager 없이)
      # ===============================================
      - name: Install AWS Load Balancer Controller without cert-manager
        run: |
          echo "🔧 AWS Load Balancer Controller 설치 중 (cert-manager 없이)..."

          # IAM Role 확인
          echo "🔍 IAM Role 확인 중..."
          PRIMARY_ROLE_NAME="${{ env.EKS_CLUSTER_NAME }}-aws-load-balancer-controller"
          
          if aws iam get-role --role-name "$PRIMARY_ROLE_NAME" >/dev/null 2>&1; then
            IAM_ROLE_ARN="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/$PRIMARY_ROLE_NAME"
            echo "✅ IAM Role 발견: $PRIMARY_ROLE_NAME"
          else
            # 백업 패턴 확인
            BACKUP_PATTERNS=("walb-eks-cluster-aws-load-balancer-controller" "walb-app-aws-load-balancer-controller")
            FOUND_ROLE=""
            for ROLE_NAME in "${BACKUP_PATTERNS[@]}"; do
              if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
                IAM_ROLE_ARN="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/$ROLE_NAME"
                FOUND_ROLE="$ROLE_NAME"
                echo "✅ IAM Role 발견: $ROLE_NAME"
                break
              fi
            done
            
            if [ -z "$FOUND_ROLE" ]; then
              echo "❌ AWS Load Balancer Controller IAM Role을 찾을 수 없습니다."
              echo "예상 Role 이름: $PRIMARY_ROLE_NAME"
              exit 1
            fi
          fi

          # VPC ID 조회
          VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.vpcId" --output text)
          echo "VPC ID: $VPC_ID"

          # ServiceAccount 생성/업데이트
          echo "🔧 ServiceAccount 생성 중..."
          kubectl delete serviceaccount aws-load-balancer-controller -n kube-system 2>/dev/null || true
          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: aws-load-balancer-controller
            namespace: kube-system
            annotations:
              eks.amazonaws.com/role-arn: $IAM_ROLE_ARN
          EOF

          # Helm repo 추가
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update

          # 기존 AWS Load Balancer Controller 완전 제거
          echo "🧹 기존 AWS Load Balancer Controller 및 웹훅 완전 제거 중..."
          helm uninstall aws-load-balancer-controller -n kube-system 2>/dev/null || true
          
          # 모든 웹훅 제거
          kubectl delete validatingadmissionwebhook vingress.elbv2.k8s.aws 2>/dev/null || true
          kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
          kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
          kubectl delete mutatingadmissionwebhook mingress.elbv2.k8s.aws 2>/dev/null || true
          
          # 웹훅 서비스 제거
          kubectl delete service aws-load-balancer-webhook-service -n kube-system 2>/dev/null || true
          
          echo "⏳ 리소스 정리 대기 (30초)..."
          sleep 30

          # AWS Load Balancer Controller 설치 (웹훅 완전 비활성화 + 네트워크 문제 회피)
          echo "🚀 AWS Load Balancer Controller 설치 (웹훅 완전 비활성화)..."
          
          # 최신 차트 버전 확인 및 설치
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ secrets.AWS_REGION }} \
            --set vpcId=$VPC_ID \
            --set enableShield=false \
            --set enableWaf=false \
            --set enableWafv2=false \
            --set webhook.enabled=false \
            --set certManager.enabled=false \
            --set enableCertManager=false \
            --set webhook.certManager.enabled=false \
            --set ingressClass=alb \
            --set createIngressClassResource=true \
            --set replicaCount=1 \
            --set resources.limits.cpu=200m \
            --set resources.limits.memory=500Mi \
            --set resources.requests.cpu=100m \
            --set resources.requests.memory=200Mi \
            --set podDisruptionBudget.maxUnavailable=1 \
            --timeout=300s \
            --wait

          # Controller Pod 상태 확인
          echo "⏳ AWS Load Balancer Controller Pod 시작 대기 중..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s

          # Controller 상태 확인 및 웹훅 상태 모니터링
          echo "🔍 AWS Load Balancer Controller 상태 확인 중..."
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          
          # 웹훅 상태 확인 및 제거 확인
          echo "🔍 웹훅 상태 확인..."
          echo "ValidatingAdmissionWebhook 목록:"
          kubectl get validatingadmissionwebhook | grep -E "(aws-load-balancer|elbv2)" || echo "✅ AWS Load Balancer 관련 웹훅 없음"
          
          echo "MutatingAdmissionWebhook 목록:"
          kubectl get mutatingadmissionwebhook | grep -E "(aws-load-balancer|elbv2)" || echo "✅ AWS Load Balancer 관련 웹훅 없음"
          
          # Controller 로그 확인
          echo "🔍 Controller 로그 확인..."
          kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=20 || true
          
          echo "✅ AWS Load Balancer Controller 준비 완료"

      # ===============================================
      # Kubernetes 매니페스트 파일 생성
      # ===============================================
      - name: Generate Kubernetes manifests
        run: |
          echo "📝 Kubernetes 매니페스트 생성 중..."

          # Namespace 생성
          cat <<EOF > namespace.yaml
          apiVersion: v1
          kind: Namespace
          metadata:
            name: ${{ env.PROJECT_NAME }}
            labels:
              name: ${{ env.PROJECT_NAME }}
          EOF

          # ConfigMap 생성 (환경변수)
          cat <<EOF > configmap.yaml
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: ${{ env.PROJECT_NAME }}-config
            namespace: ${{ env.PROJECT_NAME }}
          data:
            DB_HOST: "${{ env.RDS_ENDPOINT }}"
            DB_PORT: "5432"
            DB_NAME: "mydb"
            DB_USER: "dbadmin"
            AWS_REGION: "${{ secrets.AWS_REGION }}"
            AWS_S3_BUCKET: "walb-app-files"
            AWS_S3_REGION: "${{ secrets.AWS_REGION }}"
            STORAGE_TYPE: "s3"
            APP_ENV: "production"
            APP_DEBUG: "false"
            PHP_MEMORY_LIMIT: "256M"
            PHP_MAX_EXECUTION_TIME: "30"
            PHP_TIMEZONE: "Asia/Seoul"
            UPLOAD_MAX_SIZE: "10M"
            SESSION_LIFETIME: "7200"
          EOF

          # Secret 생성 (DB 패스워드)
          cat <<EOF > secret.yaml
          apiVersion: v1
          kind: Secret
          metadata:
            name: ${{ env.PROJECT_NAME }}-secret
            namespace: ${{ env.PROJECT_NAME }}
          type: Opaque
          data:
            DB_PASSWORD: $(echo -n "${{ secrets.DB_PASSWORD }}" | base64)
          EOF

          # Deployment 생성
          cat <<EOF > deployment.yaml
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: ${{ env.PROJECT_NAME }}-app
            namespace: ${{ env.PROJECT_NAME }}
            labels:
              app: ${{ env.PROJECT_NAME }}-app
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: ${{ env.PROJECT_NAME }}-app
            template:
              metadata:
                labels:
                  app: ${{ env.PROJECT_NAME }}-app
              spec:
                serviceAccountName: ${{ env.PROJECT_NAME }}-service-account
                containers:
                - name: php-app
                  image: ${{ steps.build-image.outputs.image }}
                  ports:
                  - containerPort: 80
                    name: http
                  envFrom:
                  - configMapRef:
                      name: ${{ env.PROJECT_NAME }}-config
                  - secretRef:
                      name: ${{ env.PROJECT_NAME }}-secret
                  livenessProbe:
                    httpGet:
                      path: /healthcheck.php
                      port: 80
                    initialDelaySeconds: 60
                    periodSeconds: 30
                    timeoutSeconds: 10
                  readinessProbe:
                    httpGet:
                      path: /healthcheck.php
                      port: 80
                    initialDelaySeconds: 30
                    periodSeconds: 10
                    timeoutSeconds: 5
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "250m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
                  securityContext:
                    runAsNonRoot: false
                    allowPrivilegeEscalation: false
                    readOnlyRootFilesystem: false
          EOF

          # Service 생성 (ClusterIP로 유지 - Ingress가 사용)
          cat <<EOF > service.yaml
          apiVersion: v1
          kind: Service
          metadata:
            name: ${{ env.PROJECT_NAME }}-service
            namespace: ${{ env.PROJECT_NAME }}
            labels:
              app: ${{ env.PROJECT_NAME }}-app
          spec:
            type: ClusterIP
            ports:
            - port: 80
              targetPort: 80
              protocol: TCP
              name: http
            selector:
              app: ${{ env.PROJECT_NAME }}-app
          EOF

          # VPC 서브넷 정보 조회
          echo "🔍 VPC 서브넷 정보 조회 중..."
          PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.vpcId" --output text)" \
                      "Name=tag:Name,Values=*public*" \
            --query "Subnets[*].SubnetId" --output text | tr '\t' ',')

          if [ -z "$PUBLIC_SUBNETS" ]; then
            # 태그 기반 조회가 실패하면 EKS 서브넷 사용
            PUBLIC_SUBNETS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.subnetIds" --output text | tr '\t' ',')
          fi

          echo "사용할 서브넷: $PUBLIC_SUBNETS"

          # Service LoadBalancer 생성 (Ingress 대안)
          cat <<EOF > service-loadbalancer.yaml
          apiVersion: v1
          kind: Service
          metadata:
            name: ${{ env.PROJECT_NAME }}-loadbalancer
            namespace: ${{ env.PROJECT_NAME }}
            annotations:
              # AWS Load Balancer 설정
              service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
              service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
              service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
              service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
              service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/healthcheck.php"
              service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "80"
              service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: "http"
              service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "30"
              service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "5"
              service.beta.kubernetes.io/aws-load-balancer-healthy-threshold: "2"
              service.beta.kubernetes.io/aws-load-balancer-unhealthy-threshold: "3"
              # 태그 설정
              service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: |
                Environment=${{ env.PROJECT_NAME }},Project=${{ env.PROJECT_NAME }},ManagedBy=Kubernetes
          spec:
            type: LoadBalancer
            ports:
            - port: 80
              targetPort: 80
              protocol: TCP
              name: http
            selector:
              app: ${{ env.PROJECT_NAME }}-app
          EOF

          # IngressClass 생성 (백업용)
          cat <<EOF > ingressclass.yaml
          apiVersion: networking.k8s.io/v1
          kind: IngressClass
          metadata:
            name: alb
            annotations:
              ingressclass.kubernetes.io/is-default-class: "true"
          spec:
            controller: ingress.k8s.aws/alb
          EOF

          # Ingress 생성 (웹훅 우회 모드 - 단순화된 설정)
          cat <<EOF > ingress-simple.yaml
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: ${{ env.PROJECT_NAME }}-ingress-simple
            namespace: ${{ env.PROJECT_NAME }}
            annotations:
              # 최소한의 ALB 설정으로 웹훅 오류 최소화
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/load-balancer-name: ${{ env.PROJECT_NAME }}-simple-alb
          spec:
            ingressClassName: alb
            rules:
            - http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: ${{ env.PROJECT_NAME }}-service
                      port:
                        number: 80
          EOF

          # ServiceAccount 생성 (IRSA용)
          cat <<EOF > serviceaccount.yaml
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: ${{ env.PROJECT_NAME }}-service-account
            namespace: ${{ env.PROJECT_NAME }}
            annotations:
              eks.amazonaws.com/role-arn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/walb-app-eks-app-role
          EOF

          echo "✅ 매니페스트 파일 생성 완료"

      # ===============================================
      # EKS에 애플리케이션 배포 (main 브랜치일 때만)
      # ===============================================
      - name: Deploy to EKS
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "🚀 EKS에 애플리케이션 배포 중..."

          # Namespace 먼저 생성
          kubectl apply -f namespace.yaml

          # 나머지 리소스 배포
          kubectl apply -f serviceaccount.yaml
          kubectl apply -f configmap.yaml
          kubectl apply -f secret.yaml
          kubectl apply -f deployment.yaml
          kubectl apply -f service.yaml

          # 1단계: Service LoadBalancer 배포 (우선 시도)
          echo "🔗 Service LoadBalancer 배포 중 (Ingress 대안)..."
          if kubectl apply -f service-loadbalancer.yaml; then
            echo "✅ Service LoadBalancer 배포 성공"
            
            # LoadBalancer 준비 대기
            echo "⏳ LoadBalancer 준비 대기 중..."
            for i in {1..12}; do
              LB_HOSTNAME=$(kubectl get service ${{ env.PROJECT_NAME }}-loadbalancer -n ${{ env.PROJECT_NAME }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              if [ -n "$LB_HOSTNAME" ]; then
                echo "✅ LoadBalancer 준비 완료: $LB_HOSTNAME"
                echo "LB_HOSTNAME=$LB_HOSTNAME" >> $GITHUB_ENV
                echo "DEPLOYMENT_METHOD=LoadBalancer" >> $GITHUB_ENV
                break
              fi
              echo "LoadBalancer 준비 대기 중... ($i/12)"
              sleep 15
            done
            
            if [ -z "$LB_HOSTNAME" ]; then
              echo "⚠️ LoadBalancer 준비 시간 초과, Ingress 시도로 전환..."
            else
              echo "✅ Service LoadBalancer 배포 완료 - Ingress 단계 건너뛰기"
              echo "SKIP_INGRESS=true" >> $GITHUB_ENV
            fi
          else
            echo "❌ Service LoadBalancer 배포 실패, Ingress 시도로 전환..."
          fi
          
          # 2단계: LoadBalancer 실패 시에만 Ingress 시도
          if [ "${SKIP_INGRESS:-false}" != "true" ]; then
            echo "🔗 Ingress 배포 시도 (단순화된 설정)..."
            
            # IngressClass 배포
            kubectl apply -f ingressclass.yaml || true
            sleep 5
            
            # 단순화된 Ingress 배포 (웹훅 우회)
            echo "단순화된 Ingress 배포 시도..."
            if kubectl apply -f ingress-simple.yaml --validate=false --force=true; then
              echo "✅ 단순화된 Ingress 배포 성공"
              
              # Ingress 상태 확인
              sleep 15
              if kubectl get ingress ${{ env.PROJECT_NAME }}-ingress-simple -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
                echo "✅ Ingress 리소스 생성 확인"
                kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress-simple -n ${{ env.PROJECT_NAME }}
                echo "DEPLOYMENT_METHOD=Ingress" >> $GITHUB_ENV
              else
                echo "❌ Ingress 리소스 생성 실패"
                echo "⚠️ LoadBalancer와 Ingress 모두 실패 - 포트포워딩으로 대체"
                echo "DEPLOYMENT_METHOD=PortForward" >> $GITHUB_ENV
              fi
            else
              echo "❌ Ingress 배포 실패"
              echo "⚠️ LoadBalancer와 Ingress 모두 실패 - 포트포워딩으로 대체"
              echo "DEPLOYMENT_METHOD=PortForward" >> $GITHUB_ENV
              
              # 진단 정보 수집
              echo "진단 정보:"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
              kubectl get validatingadmissionwebhook | grep -E "(vingress|aws-load-balancer)" || echo "웹훅 없음"
              kubectl get ingressclass || true
            fi
          fi

          echo "⏳ 배포 완료 대기 중..."
          kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s

      # ===============================================
      # 배포 결과 확인
      # ===============================================
      - name: Verify Deployment
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "🔍 배포 상태 확인 중..."
          echo "네임스페이스: ${{ env.PROJECT_NAME }}"
          echo "앱 라벨: ${{ env.PROJECT_NAME }}-app"

          # 네임스페이스 존재 확인
          if ! kubectl get namespace ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "❌ 네임스페이스 '${{ env.PROJECT_NAME }}'가 존재하지 않습니다."
            kubectl get namespaces
            exit 1
          fi

          # 기본 리소스 상태 확인
          echo "📋 Pod 상태:"
          kubectl get pods -n ${{ env.PROJECT_NAME }} -o wide || echo "Pod를 찾을 수 없습니다."

          echo "📋 Service 상태:"
          kubectl get services -n ${{ env.PROJECT_NAME }} || echo "Service를 찾을 수 없습니다."

          echo "📋 Deployment 상태:"
          kubectl get deployments -n ${{ env.PROJECT_NAME }} || echo "Deployment를 찾을 수 없습니다."

          echo "📋 Ingress 상태:"
          kubectl get ingress -n ${{ env.PROJECT_NAME }} || echo "Ingress를 찾을 수 없습니다."

          echo "📋 AWS Load Balancer Controller 상태:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || echo "AWS Load Balancer Controller를 찾을 수 없습니다."

          # Deployment 존재 확인 후 대기
          if kubectl get deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "⏳ Pod 준비 상태 대기 중..."
            kubectl wait --for=condition=ready pod -l app=${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || echo "Pod 준비 상태 대기 시간 초과"
            
            echo "📝 Deployment 상세 정보:"
            kubectl describe deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }}
            
          else
            echo "❌ Deployment '${{ env.PROJECT_NAME }}-app'를 찾을 수 없습니다."
            echo "사용 가능한 Deployment 목록:"
            kubectl get deployments -n ${{ env.PROJECT_NAME }}
            exit 1
          fi

      # ===============================================
      # 배포 방법별 접속 URL 제공
      # ===============================================
      - name: Get Application URL
        if: success() && github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "🔗 애플리케이션 접속 정보 확인 중..."
          echo "배포 방법: ${DEPLOYMENT_METHOD:-Unknown}"

          if [ "${DEPLOYMENT_METHOD:-}" = "LoadBalancer" ]; then
            echo "🌐 Service LoadBalancer로 배포됨"
            echo "애플리케이션 접속 URL: http://${LB_HOSTNAME:-확인불가}"
            echo "🔍 LoadBalancer 서비스 상태:"
            kubectl describe service ${{ env.PROJECT_NAME }}-loadbalancer -n ${{ env.PROJECT_NAME }}
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress" ]; then
            echo "🔗 Ingress로 배포됨"
            # Ingress에서 ALB DNS 확인
            for i in {1..12}; do
              ALB_DNS=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress-simple -n ${{ env.PROJECT_NAME }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              if [ -n "$ALB_DNS" ]; then
                echo "🌐 애플리케이션 접속 URL: http://$ALB_DNS"
                break
              fi
              echo "ALB DNS 대기 중... ($i/12)"
              sleep 10
            done
            
            if [ -z "$ALB_DNS" ]; then
              echo "⚠️ ALB DNS를 아직 찾을 수 없습니다."
            fi
            
            echo "🔍 Ingress 상태:"
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress-simple -n ${{ env.PROJECT_NAME }}
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "PortForward" ]; then
            echo "⚠️ LoadBalancer와 Ingress가 모두 실패하여 포트포워딩 사용"
            echo "로컬 접근 방법:"
            echo "kubectl port-forward -n ${{ env.PROJECT_NAME }} svc/${{ env.PROJECT_NAME }}-service 8080:80"
            echo "그 후 브라우저에서 http://localhost:8080 접근"
            
          else
            echo "❓ 배포 방법을 확인할 수 없습니다."
            echo "사용 가능한 서비스 확인:"
            kubectl get services -n ${{ env.PROJECT_NAME }}
            echo "사용 가능한 Ingress 확인:"
            kubectl get ingress -n ${{ env.PROJECT_NAME }}
          fi

          # 공통 디버깅 정보
          echo ""
          echo "📋 전체 리소스 상태:"
          kubectl get all -n ${{ env.PROJECT_NAME }}

      # ===============================================
      # 배포 완료 알림
      # ===============================================
      - name: Application Deployment Notification
        if: success() && github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "🎉 PHP 애플리케이션 배포 완료!"
          echo "프로젝트: ${{ env.PROJECT_NAME }}"
          echo "이미지: ${{ steps.build-image.outputs.image }}"
          echo "클러스터: ${{ env.EKS_CLUSTER_NAME }}"
          echo "네임스페이스: ${{ env.PROJECT_NAME }}"
          echo "데이터베이스: ${{ env.RDS_ENDPOINT }}"
          echo "커밋: ${{ github.sha }}"
          echo "배포 시간: $(date)"

          # 배포된 서비스 정보 출력
          echo "📋 배포된 리소스 목록:"
          kubectl get all -n ${{ env.PROJECT_NAME }} || echo "리소스 조회 실패"

      # ===============================================
      # 배포 실패 시 롤백
      # ===============================================
      - name: Rollback on failure
        if: failure() && github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "❌ 배포 실패 - 이전 버전으로 롤백 중..."
          kubectl rollout undo deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} || true
          kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || true
          echo "✅ 롤백 시도 완료"
